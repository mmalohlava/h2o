package hex.rf;

import hex.rf.ConfusionTask.CMFinal;
import hex.rf.DRF.DRFJob;
import hex.rf.DRF.DRFParams;
import hex.rf.RefinedTree.*;
import hex.rf.Tree.StatType;
import hex.rng.H2ORandomRNG.RNGKind;

import java.io.File;
import java.util.*;

import jsr166y.ForkJoinTask;
import water.*;
import water.Timer;
import water.util.*;
import water.util.Log.Tag.Sys;

/**
 * A RandomForest can be used for growing or validation. The former starts with a known target number of trees,
 * the latter is incrementally populated with trees as they are built.
 * Validation and error reporting is not supported when growing a forest.
 */
public class RandomForest {

  /** Seed initializer.
   *  Generated by:  cat /dev/urandom | tr -dc '0-9a-f' | fold -w 16| head -n 1' */
  private static final long ROOT_SEED_ADD  = 0x026244fd935c5111L;
  private static final long TREE_SEED_INIT = 0x1321e74a0192470cL;

  /** Build random forest for data stored on this node. */
  public static Tree[] build(
                      final Job job,
                      final DRFParams drfParams,
                      final Data data,
                      int ntrees,
                      int numSplitFeatures) {
    Timer  t_alltrees = new Timer();
    Tree[] trees      = new Tree[ntrees];
    Log.debug(Sys.RANDF,"Building "+ntrees+" trees");
    Log.debug(Sys.RANDF,"Number of split features: "+ numSplitFeatures);
    Log.debug(Sys.RANDF,"Starting RF computation with "+ data.rows()+" rows ");

    Random  rnd     = Utils.getRNG(data.seed() + ROOT_SEED_ADD);
    Sampling sampler = createSampler(drfParams);
    for (int i = 0; i < ntrees; ++i) {
      long treeSeed = rnd.nextLong() + TREE_SEED_INIT; // make sure that enough bits is initialized
      trees[i] = new Tree(job, data, drfParams._depth, drfParams._stat, numSplitFeatures, treeSeed,
                          i, drfParams._exclusiveSplitLimit, sampler, drfParams._verbose, drfParams._nodesize);
      if (!drfParams._parallel) ForkJoinTask.invokeAll(new Tree[]{trees[i]});
    }
    // Invoking in parallel at this node
    if(drfParams._parallel) ForkJoinTask.invokeAll(trees);
    Log.debug(Sys.RANDF,"All trees ("+ntrees+") done in "+ t_alltrees);

    return trees;
  }

  public static void refineByRotating(final Job job, final DRFParams drfParams, final Data data, int ntrees, int numSplitFeatures) {
    final int selfNIdx = H2O.SELF.index();
    RFModel m = UKV.get(job.dest());
    final Key[] selfRF = m._localForests[selfNIdx];
    sendToNextNode(job, selfRF); DKV.write_barrier();

    // 2) look into my queue for trees for refinement
    Sampling sampler = createSampler(drfParams);
    int indx = 0;
    LinkedList<RefinedTree> ops = new LinkedList<RefinedTree>();
    LinkedList<RefinedTree> lops = new LinkedList<RefinedTree>();
    int cnt = 0;
    int totalTrees = m._totalTrees - ntrees;
    while (cnt < totalTrees /*+ ntrees*/) {
      ops.clear();
      lops.clear();
      m = UKV.get(job.dest()); // get RF model view
      Key[] selfRQueue = m._refineQueues[selfNIdx];
      for(int i=indx; i<selfRQueue.length; i++, indx++) {
        Key treeToRefine = selfRQueue[i];
        if (!m.isTreeFromNode(treeToRefine, selfNIdx)) { // it is not my own tree comming back after going around the cloud
          byte[] serialTree = DKV.get(treeToRefine).memOrLoad();
          long seed  = Tree.seed(serialTree);
          int treeId = Tree.treeId(serialTree);
          ops.add(new RefinedTree(RefinedTree.UPDATE_KEY_ACTION, job,treeToRefine, new AutoBuffer(serialTree), -1, treeId, treeId, seed, data, drfParams._depth, drfParams._stat, numSplitFeatures, drfParams._exclusiveSplitLimit, sampler, drfParams._verbose, drfParams._nodesize));
          cnt++;
        } else { // it is a local node
//          byte[] serialTree = DKV.get(treeToRefine).memOrLoad();
//          long seed  = Tree.seed(serialTree);
//          int treeId = Tree.treeId(serialTree);
//          lops.add(new RefinedTree(RefinedTree.UPDATE_KEY_ACTION, job,treeToRefine, new AutoBuffer(serialTree), -1, treeId, treeId, seed, data, drfParams._depth, drfParams._stat, numSplitFeatures, drfParams._exclusiveSplitLimit, sampler, drfParams._verbose, drfParams._nodesize));
//          cnt++;
        }
      }
      if (!ops.isEmpty()) {
        ForkJoinTask.invokeAll(ops); // refine tree
        // send refined trees to next node
        Key keys[] = new Key[ops.size()];
        for(int i=0;i<ops.size();i++) keys[i] = ops.get(i)._origTreeKey;
        sendToNextNode(job, keys);
        DKV.write_barrier();
      } else if (!lops.isEmpty()) {
        ForkJoinTask.invokeAll(lops); // refine tree
        DKV.write_barrier();
      } else { // or wait little bit for rest of nodes
        try { Thread.sleep(500); } catch (InterruptedException _) {};
      }
    }
  }

  static void sendToNextNode(final Job job, final Key...keys) {
    final int selfNIdx = H2O.SELF.index();
    final int targetNIdx = (selfNIdx + 1) % H2O.CLOUD.size(); // FIXME not all nodes participates in DRF !

    // 1) Transfer my local trees into next node for refinement
    new TAtomic<RFModel>() {
      @Override public RFModel atomic(RFModel old) {
        return RFModel.updateRQ(old, targetNIdx, keys);
      }
    }.invoke(job.dest());
  }

  public static void refineByMerging(final Job job, final DRFParams drfParams, final Data data, int ntrees, int numSplitFeatures) {
    final Timer timerRefinement = new Timer();
    int idx = 0;
    int nodeIdx = H2O.SELF.index();
    Sampling sampler = createSampler(drfParams);

    Log.info("Starting refinement...");
    while (idx < drfParams._ntrees-ntrees) {
      RFModel m = UKV.get(job.dest());
      Key[][] refinedForests = m._refinedForests[nodeIdx]; // this is a forest of trees which this node refines
      LinkedList<RefinedTree> trees = new LinkedList<RefinedTree>();
      for (int i=0; i<refinedForests.length; i++) { // do loop over all nodes in cluster
        if (i == nodeIdx)  continue;  // do not refine self
        Key[] targetNodeForest = m._localForests[i];
        for (int j=0; j<targetNodeForest.length; j++) {
          // Check if the tree was already generated and refined
          if (targetNodeForest[j]!=null && (j>=refinedForests[i].length || refinedForests[i][j]==null)) {
            Key tKey = targetNodeForest[j];
            byte[] serialTree = DKV.get(tKey).memOrLoad();
            long seed = Tree.seed(serialTree);
            int treeId = Tree.treeId(serialTree);
            trees.add(new RefinedTree(RefinedTree.UPDATE_RTM, job, tKey, new AutoBuffer(serialTree), i, j, treeId, seed, data, drfParams._depth, drfParams._stat, numSplitFeatures, drfParams._exclusiveSplitLimit, sampler, drfParams._verbose, drfParams._nodesize));
            idx++;
          }
        }
      }
      if (!trees.isEmpty()) {
        ForkJoinTask.invokeAll(trees); // refine tree
      } else { // or wait little bit for rest of nodes
        try { Thread.sleep(500); } catch (InterruptedException _) {};
      }
    }
    DKV.write_barrier();
    Log.info("Refinement done on node: " + nodeIdx + " in: " + timerRefinement);

    switch( drfParams._refineStrategy ) {
      case MERGE : refineMerge (job, drfParams, ntrees); break;
      case APPEND: refineAppend(job, drfParams, ntrees); break;
      default: break;
    }
  }

  // Append refined trees produced by this node to a global RFModel
  private static void refineAppend(Job job, DRFParams drfParams, int ntrees) {
    final Timer timerAppend = new Timer();
    int nodeIdx = H2O.SELF.index();
    RFModel m = UKV.get(job.dest());
    final Key[][] refinedForests = m._refinedForests[nodeIdx];
    final int numOfTrees = drfParams._ntrees - ntrees;
    final Key[] allRefinedTrees = new Key[numOfTrees];
    int idx = 0;
    Log.info("Starting refineAppend - refined tree by node " + nodeIdx);
    for(int i=0; i< refinedForests.length;i++) {
      Log.debug("> Node " + i);
      for (int j=0; j<refinedForests[i].length; j++) {
        Log.debug(">> Tree: " + refinedForests[i][j]);
        allRefinedTrees[idx++] = refinedForests[i][j];
      }
    }
    new TAtomic<RFModel>() {
      @Override public RFModel atomic(RFModel old) {
        RFModel newm = old.clone();
        int len = newm._tkeys.length;
        newm._tkeys = Arrays.copyOf(newm._tkeys, len + allRefinedTrees.length);
        for (int i=0; i<allRefinedTrees.length;i++) newm._tkeys[len + i] = allRefinedTrees[i];
        newm._totalTrees += allRefinedTrees.length;

        return newm;
      }
    }.invoke(job.dest());
    DKV.write_barrier();
    Log.info("Append done in " + timerAppend);
  }

  // Merge this node trees.
  // For each my tree, try to merge it with all its refined versions produced by other nodes.
  private static void refineMerge(Job job, DRFParams drfParams, int ntrees) {
    final Timer timerMerge = new Timer();
    final int nodeIdx = H2O.SELF.index();
    final int totalNodes = H2O.CLOUD.size();
    final int numOfTrees = (totalNodes-1)*ntrees; // Number of trees to merge
    int[] sindx = new int[ntrees]; // list indices of node which should be merged as next
    List<MergeTreesOp> ops = new ArrayList<MergeTreesOp>(ntrees);
    Log.info("Starting refineMerge...");

    int tcnt = 0;
    while (tcnt < numOfTrees) {
      RFModel rfModel = UKV.get(job.dest());
      ops.clear();
      for(int i=0; i<sindx.length; i++) {
        int rnIdx = sindx[i];
        if (rnIdx==nodeIdx) rnIdx++; // skip this node
        if (rnIdx < totalNodes) {
          Key masterTKey = rfModel._localForests[nodeIdx][i]; // The i-th which this node produced
          Key[][] refinedForest = rfModel._refinedForests[rnIdx]; // Foreign node forest
          if (refinedForest[nodeIdx]!=null && i<refinedForest[nodeIdx].length && refinedForest[nodeIdx][i]!=null) {
            Log.info("Merging master tree " + i + " from node " + nodeIdx + " with tree from node " + rnIdx);
            Key refinedTKey = rfModel._refinedForests[rnIdx][nodeIdx][i];
            ops.add(new MergeTreesOp(job.dest(), masterTKey, refinedTKey));
            sindx[i] = ++rnIdx;
          }
        }
      }
      if (!ops.isEmpty()) {
        ForkJoinTask.invokeAll(ops);
      } else { // or wait little bit for rest of nodes
        try { Thread.sleep(500); } catch (InterruptedException _) {};
      }
      tcnt += ops.size();
    }
    DKV.write_barrier();
    Log.info("Merge done in " + timerMerge);
  }

  static Sampling createSampler(final DRFParams params) {
    switch(params._samplingStrategy) {
    case RANDOM          : return new Sampling.Random(params._sample, params._numrows);
    case STRATIFIED_LOCAL:
      float[] ss = new float[params._strataSamples.length];
      for (int i=0;i<ss.length;i++) ss[i] = params._strataSamples[i] / 100.f;
      return new Sampling.StratifiedLocal(ss, params._numrows);
    default:
      assert false : "Unsupported sampling strategy";
      return null;
    }
  }

  public static class OptArgs extends Arguments.Opt {
    String  file          = "smalldata/poker/poker-hand-testing.data";
    String  rawKey;
    String  parsedKey;
    String  validationFile;
    String  h2oArgs;
    int     ntrees        = 10;
    int     depth         = Integer.MAX_VALUE;
    int     sample        = 67;
    int     binLimit      = 1024;
    int     classcol      = -1;
    int     features      = -1;
    int     parallel      = 1;
    int     nodesize      = 1;
    boolean outOfBagError = true;
    boolean stratify      = false;
    String  strata;
    String  weights;
    String  statType      = "entropy";
    long    seed          = 0xae44a87f9edf1cbL;
    String  ignores;
    int     cloudFormationTimeout = 10; // wait for up to 10seconds
    int     verbose       = 0; // levels of verbosity
    int     exclusive     = 0; // exclusive split limit, 0 = exclusive split is disabled
    String  rng           = RNGKind.DETERMINISTIC.name();
  }

  static final OptArgs ARGS = new OptArgs();

  public static Map<Integer,Integer> parseStrata(String s){
    if(s.isEmpty())return null;
    String [] strs = s.split(",");
    Map<Integer,Integer> res = new HashMap<Integer, Integer>();
    for(String x:strs){
      String [] arr = x.split(":");
      res.put(Integer.parseInt(arr[0].trim()), Integer.parseInt(arr[1].trim()));
    }
    return res;
  }

  public static void main(String[] args) throws Exception {
    Arguments arguments = new Arguments(args);
    arguments.extract(ARGS);
    String[] h2oArgs;
    if(ARGS.h2oArgs == null) { // By default run using local IP, C.f. JUnitRunner
      File flat = Utils.writeFile("127.0.0.1:54327");
      h2oArgs = new String[] { "-ip=127.0.0.1", "-flatfile=" + flat.getAbsolutePath() };
    } else {
      if(ARGS.h2oArgs.startsWith("\"") && ARGS.h2oArgs.endsWith("\""))
        ARGS.h2oArgs = ARGS.h2oArgs.substring(1, ARGS.h2oArgs.length()-1);
      ARGS.h2oArgs = ARGS.h2oArgs.trim();
      h2oArgs = ARGS.h2oArgs.split("[ \t]+");
    }
    H2O.main(h2oArgs);
    ValueArray va;
    // get the input data
    if(ARGS.parsedKey != null) // data already parsed
      va = DKV.get(Key.make(ARGS.parsedKey)).get();
    else if(ARGS.rawKey != null) // data loaded in K/V, not parsed yet
      va = TestUtil.parse_test_key(Key.make(ARGS.rawKey),Key.make(TestUtil.getHexKeyFromRawKey(ARGS.rawKey)));
    else { // data outside of H2O, load and parse
      File f = new File(ARGS.file);
      Log.debug(Sys.RANDF,"Loading file ", f);
      Key fk = TestUtil.load_test_file(f);
      va = TestUtil.parse_test_key(fk,Key.make(TestUtil.getHexKeyFromFile(f)));
      DKV.remove(fk);
    }
    if(ARGS.ntrees == 0) {
      Log.warn(Sys.RANDF,"Nothing to do as ntrees == 0");
      UDPRebooted.T.shutdown.broadcast();
      return;
    }
    StatType st = ARGS.statType.equals("gini") ? StatType.GINI : StatType.ENTROPY;

    Map<Integer,Integer> strata = (ARGS.stratify && ARGS.strata != null) ? parseStrata(ARGS.strata) : null;

    double[] classWeights = null;
    if(ARGS.stratify && ARGS.strata != null) {
      Map<Integer,Integer> weights = parseStrata(ARGS.weights);
      int[] ks = new int[weights.size()];
      int i=0; for (Object clss : weights.keySet().toArray()) ks[i++]= (Integer)clss;
      Arrays.sort(ks);
      classWeights = new double[ks.length];
      i=0; for(int k : ks) classWeights[i++] = weights.get(k);
    }

    // Setup desired random generator.
    Utils.setUsedRNGKind(RNGKind.value(ARGS.rng));

    final int num_cols = va._cols.length;
    final int classcol = ARGS.classcol == -1 ? num_cols-1: ARGS.classcol; // Defaults to last column

    // Build the set of positive included columns
    BitSet bs = new BitSet();
    bs.set(0,va._cols.length);
    bs.clear(classcol);
    if (ARGS.ignores!=null)
      for( String s : ARGS.ignores.split(",") )
        bs.clear(Integer.parseInt(s));
    int cols[] = new int[bs.cardinality()+1];
    int idx=0;
    for( int i=bs.nextSetBit(0); i >= 0; i=bs.nextSetBit(i+1))
      cols[idx++] = i;
    cols[idx++] = classcol;     // Class column last
    assert idx==cols.length;
    assert ARGS.sample >0 && ARGS.sample<=100;
    assert ARGS.ntrees >=0;
    assert ARGS.binLimit > 0 && ARGS.binLimit <= Short.MAX_VALUE;

    Log.debug(Sys.RANDF,"Arguments used:\n"+ARGS.toString());
    final Key modelKey = Key.make("model");
    DRFJob drfJob = DRF.execute(modelKey,
                          cols,
                          va,
                          ARGS.ntrees,
                          ARGS.depth,
                          ARGS.binLimit,
                          st,
                          ARGS.seed,
                          ARGS.parallel==1,
                          classWeights,
                          ARGS.features, // number of split features or -1 (default)
                          ARGS.stratify ? Sampling.Strategy.STRATIFIED_LOCAL : Sampling.Strategy.RANDOM,
                          (ARGS.sample/100.0f),
                          /* FIXME strata*/ null,
                          ARGS.verbose,
                          ARGS.exclusive,
                          false,
                          ARGS.nodesize,
                          false, null);
    RFModel model = drfJob.get();  // block on all nodes!
    Log.debug(Sys.RANDF,"Random forest finished in TODO"/*+ drf._t_main*/);

    Timer t_valid = new Timer();
    // Get training key.
    Key valKey = model._dataKey;
    if(ARGS.outOfBagError && !ARGS.stratify){
      Log.debug(Sys.RANDF,"Computing out of bag error");
      CMFinal cm = ConfusionTask.make( model, valKey, classcol, null, true).get(); // block until CM is computed
      cm.report();
    }
    // Run validation.
    if(ARGS.validationFile != null && !ARGS.validationFile.isEmpty()){ // validate on the supplied file
      File f = new File(ARGS.validationFile);
      Log.debug(Sys.RANDF,"Loading validation file ",f);
      Key fk = TestUtil.load_test_file(f);
      ValueArray v = TestUtil.parse_test_key(fk,Key.make(TestUtil.getHexKeyFromFile(f)));
      valKey = v._key;
      DKV.remove(fk);
      CMFinal cm = ConfusionTask.make( model, valKey, classcol, null, false).get();
      cm.report();
    }

    Log.debug(Sys.RANDF,"Validation done in: " + t_valid);
    UDPRebooted.T.shutdown.broadcast();
  }
}
